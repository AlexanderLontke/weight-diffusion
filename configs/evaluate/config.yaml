exp_name: model_evaluation
seed: 1337
device: &device cuda
sampling_method: ldm #Gpt
data_dir: &data_dir /netscratch2/kreynisson/data/tune_zoo_mnist_uniform
pickled_sampled_checkpoints_dir: /netscratch2/alontke/weight-diffusion/sampled_weights/2023-01-23/16-57-24


wandb_config:
  name: ${exp_name}
  group: default
  project: WeightDiffusion
  entity: alontke
  mode: online

gpt_config:
  transformer:
    ema: true
    absolute_loss_conditioning: true
    predict_xstart: true
    chunk_size: 1000
    split_policy: "chunk_within_layer"
    max_freq_log2: 14
    num_frequencies: 128
    n_embd: 1536
    encoder_depth: 1
    decoder_depth: 1
    n_layer: 12
    n_head: 16
    dropout_prob: 0.0
  dataset:
    name: zoo_mnist
    train_metric: validation_loss
    path: "/netscratch2/kreynisson/data/tune_zoo_mnist_uniform"
    num_test_runs: 500
    augment: true
    normalizer: openai
    openai_coefficient: 4.185
    target_epoch_size: 100800
    max_train_runs: 1000000
    num_workers: 1

ldm_config:
  ldm_checkpoint_path: /netscratch2/kreynisson/weight-diffusion/logs/2022-12-09T15-56-04_hp-ldm/checkpoints/last.ckpt
  model:
    base_learning_rate: 1.0e-04
    target: ldm.models.diffusion.ddpm.LatentDiffusion
    params:
      linear_start: 0.00085
      linear_end: 0.0120
      num_timesteps_cond: 1
      log_every_t: 200
      timesteps: 1000
      first_stage_key: checkpoint_latent
      cond_stage_key: prompt_latent
      image_size: 700 # TODO check if correct
      channels: 1
      cond_stage_trainable: false
      conditioning_key: crossattn
      monitor: val/loss_simple_ema
      scale_factor: 0.18215
      use_ema: False

      scheduler_config: # 10000 warmup steps
        target: ldm.lr_scheduler.LambdaLinearScheduler
        params:
          warm_up_steps: [ 10000 ]
          cycle_lengths: [ 10000000000000 ] # incredibly large number to prevent corner cases
          f_start: [ 1.e-6 ]
          f_max: [ 1. ]
          f_min: [ 1. ]

      unet_config:
        target: ldm.modules.diffusionmodules.openaimodel.UNetModel
        params:
          image_size: 700 # unused
          in_channels: 1
          out_channels: 1
          dims: 1
          model_channels: 320
          attention_resolutions: [ 4, 2, 1 ]
          num_res_blocks: 2
          channel_mult: [ 1, 2, 4]
          num_heads: 8
          use_spatial_transformer: True
          transformer_depth: 1
          context_dim: 3
          use_checkpoint: False
          legacy: False
      first_stage_config:
        target: ldm.models.autoencoder.IdentityFirstStage
        params:
          vq_interface: False
      cond_stage_config:
        target: torch.nn.Identity

encoder_config: &encoder_config
  device: *device
  encoder_checkpoint_path: /Users/alexanderlontke/Documents/Uni/St. Gallen/HS_22_23/integrative_master_project/data_hr/hyper_representations/mnist
  #/netscratch2/kreynisson/data/hyper_representations/mnist
  target: ghrp.model_definitions.def_simclr_ae_module.SimCLRAEModule
  params:
    config:
      device: *device
      model::N_attention_blocks: 2
      model::N_attention_heads: 12
      model::attention_hidden_dim: 1380
      model::bottleneck: linear_bounded
      model::compression: token
      model::decompression: linear
      model::decoder_bias: false
      model::dim_attention_embedding: 1428
      model::dropout: 0.1
      model::encoding: neuron
      model::i_dim: 2464
      model::index_dict:
        channels_in:
          - 1
          - 8
          - 6
          - 4
          - 20
        idx_length:
          - 208
          - 1206
          - 100
          - 740
          - 210
        idx_start:
          - 0
          - 208
          - 1414
          - 1514
          - 2254
        kernel_no:
          - 8
          - 6
          - 4
          - 20
          - 10
        kernel_size:
          - 25
          - 25
          - 4
          - 9
          - 1
        layer:
          - - 0
            - conv2d
          - - 3
            - conv2d
          - - 6
            - conv2d
          - - 9
            - fc
          - - 11
            - fc
      model::latent_dim: 700
      model::normalize: true
      model::projection_head_batchnorm: false
      model::projection_head_hdim: 400
      model::projection_head_layers: 4
      model::projection_head_odim: 50
      model::type: transformer
      optim::lr: 0.0001
      optim::optimizer: adam
      optim::scheduler: ReduceLROnPlateau
      optim::scheduler_factor: 0.3
      optim::scheduler_mode: min
      optim::wd: 1.0e-09
      seed: 42
      testloader::workers: 4
      testset::add_noise_input: false
      testset::add_noise_output: false
      testset::ds_split:
        - 0.7
        - 0.15
        - 0.15
      testset::epoch_lst:
        - 21
        - 22
        - 23
        - 24
        - 25
      testset::filter_function:
      testset::layer_lst:
        - - 0
          - conv2d
        - - 3
          - conv2d
        - - 6
          - conv2d
        - - 9
          - fc
        - - 11
          - fc
      testset::mode: vector
      testset::permutation_mode: random
      testset::permutations_number: 100
      testset::permute_layers:
        - 0
        - 3
        - 6
        - 9
      testset::permute_type: pair
      testset::property_keys:
        config_keys: [ ]
        result_keys:
          - test_acc
          - training_iteration
          - ggap
      testset::task: reconstruction
      testset::use_bias: true
      testset::weight_threshold: 5
      training::checkpoint_dir:
      training::contrast: positive
      training::epochs_train: 350
      training::normalize_loss: true
      training::gamma: 0.099602290720457
      training::output_epoch: 50
      training::start_epoch: 1
      training::temperature: 0.1
      training::tensorboard_dir:
      training::test_epochs: 5
      training::tf_out: 500
      trainloader::workers: 4
      trainset::add_noise_input: false
      trainset::add_noise_output: false
      trainset::batchsize: 500
      trainset::ds_split:
        - 0.7
        - 0.15
        - 0.15
      trainset::epoch_lst:
        - 21
        - 22
        - 23
        - 24
        - 25
      trainset::erase_augment:
        mode: block
        p: 0.5
        scale:
          - 0.02
          - 0.33
        value: 0
      trainset::filter_function:
      trainset::layer_lst:
        - - 0
          - conv2d
        - - 3
          - conv2d
        - - 6
          - conv2d
        - - 9
          - fc
        - - 11
          - fc
      trainset::mode: vector
      trainset::permutation_mode: random
      trainset::permutations_number: 25000
      trainset::permute_layers:
        - 0
        - 3
        - 6
        - 9
      trainset::permute_type: pair
      trainset::property_keys:
        config_keys: [ ]
        result_keys:
          - test_acc
          - training_iteration
          - ggap
      trainset::task: reconstruction
      trainset::use_bias: true
      trainset::weight_threshold: 5
      verbosity: 0

tokenizer_config: &tokenizer_config
  target: weight_diffusion.ofga.encoders.get_pretrained_bert_tokenizer
  params:
    pretrained_model_name_or_path: tbs17/MathBERT
    output_hidden_states: True

sampling_config:
  shape:
    - 1
    - 700
  sampling_steps: [20] #[1, 20, 40, 60, 80, 100]
  data_dir: *data_dir
  prompt_embedding_max_length: 77
  evaluation_prompt_statistics:
    prompt_1:
      train_loss: 2.2193379314899446
      train_acc: 0.16126
      validation_loss: 2.076323986053467
      validation_acc: 0.1999
      test_loss: 2.072357416152954
      test_acc: 0.1995
    prompt_2:
      train_loss: 1.8915899208545686
      train_acc: 0.26984
      validation_loss: 1.877575159072876
      validation_acc: 0.2812
      test_loss: 1.8670214414596558
      test_acc: 0.2777
    prompt_3:
      train_loss: 1.7501586325645446
      train_acc: 0.33576
      validation_loss: 1.7005707025527954
      validation_acc: 0.362
      test_loss: 1.6803345680236816
      test_acc: 0.3606
    prompt_4:
      train_loss: 1.5255577799201012
      train_acc: 0.4363
      validation_loss: 1.4922499656677246
      validation_acc: 0.4555
      test_loss: 1.4733335971832275
      test_acc: 0.4653
    prompt_5:
      train_loss: 1.1893459279954433
      train_acc: 0.5671
      validation_loss: 1.1699752807617188
      validation_acc: 0.5832
      test_loss: 1.1508769989013672
      test_acc: 0.583
    prompt_6:
      train_loss: 0.9511789116233588
      train_acc: 0.65918
      validation_loss: 0.9407992362976074
      validation_acc: 0.6659
      test_loss: 0.9020390510559082
      test_acc: 0.6737
    prompt_7:
      train_loss: 0.6917352895915508
      train_acc: 0.75768
      validation_loss: 0.6968080997467041
      validation_acc: 0.7626
      test_loss: 0.6469360589981079
      test_acc: 0.7737
    prompt_8:
      train_loss: 0.4302793982863426
      train_acc: 0.86308
      validation_loss: 0.46385976672172546
      validation_acc: 0.8505
      test_loss: 0.41706815361976624
      test_acc: 0.8669
    prompt_9:
      train_loss: 0.3219652168320492
      train_acc: 0.89962
      validation_loss: 0.3643215000629425
      validation_acc: 0.8857
      test_loss: 0.3315613865852356
      test_acc: 0.901
  evaluation_prompt_statistics_ldm:
    prompt_1:
      train_acc: 0.0
      validation_acc: 0.0
      test_acc: 0.0
    prompt_2:
      train_acc: 0.0
      validation_acc: 0.0
      test_acc: 0.0
    prompt_3:
      train_acc: 0.0
      validation_acc: 0.0
      test_acc: 0.0
    prompt_4:
      train_acc: 0.0
      validation_acc: 0.0
      test_acc: 0.0
    prompt_5:
      train_acc: 0.0
      validation_acc: 0.0
      test_acc: 0.0
    prompt_21:
      train_acc: 0.2
      validation_acc: 0.2
      test_acc: 0.2
    prompt_22:
      train_acc: 0.2
      validation_acc: 0.2
      test_acc: 0.2
    prompt_23:
      train_acc: 0.2
      validation_acc: 0.2
      test_acc: 0.2
    prompt_24:
      train_acc: 0.2
      validation_acc: 0.2
      test_acc: 0.2
    prompt_25:
      train_acc: 0.2
      validation_acc: 0.2
      test_acc: 0.2
    prompt_41:
      train_acc: 0.4
      validation_acc: 0.4
      test_acc: 0.4
    prompt_42:
      train_acc: 0.4
      validation_acc: 0.4
      test_acc: 0.4
    prompt_43:
      train_acc: 0.4
      validation_acc: 0.4
      test_acc: 0.4
    prompt_44:
      train_acc: 0.4
      validation_acc: 0.4
      test_acc: 0.4
    prompt_45:
      train_acc: 0.4
      validation_acc: 0.4
      test_acc: 0.4
    prompt_61:
      train_acc: 0.6
      validation_acc: 0.6
      test_acc: 0.6
    prompt_62:
      train_acc: 0.6
      validation_acc: 0.6
      test_acc: 0.6
    prompt_63:
      train_acc: 0.6
      validation_acc: 0.6
      test_acc: 0.6
    prompt_64:
      train_acc: 0.6
      validation_acc: 0.6
      test_acc: 0.6
    prompt_65:
      train_acc: 0.6
      validation_acc: 0.6
      test_acc: 0.6
    prompt_81:
      train_acc: 0.8
      validation_acc: 0.8
      test_acc: 0.8
    prompt_82:
      train_acc: 0.8
      validation_acc: 0.8
      test_acc: 0.8
    prompt_83:
      train_acc: 0.8
      validation_acc: 0.8
      test_acc: 0.8
    prompt_84:
      train_acc: 0.8
      validation_acc: 0.8
      test_acc: 0.8
    prompt_85:
      train_acc: 0.8
      validation_acc: 0.8
      test_acc: 0.8
    prompt_101:
      train_acc: 1.0
      validation_acc: 1.0
      test_acc: 1.0
    prompt_102:
      train_acc: 1.0
      validation_acc: 1.0
      test_acc: 1.0
    prompt_103:
      train_acc: 1.0
      validation_acc: 1.0
      test_acc: 1.0
    prompt_104:
      train_acc: 1.0
      validation_acc: 1.0
      test_acc: 1.0
    prompt_105:
      train_acc: 1.0
      validation_acc: 1.0
      test_acc: 1.0


finetune_config:
  finetune_epochs: [1, 5, 25]
  prompts_to_finetune: []

evaluation_dataset_config:
  data_dir: *data_dir




